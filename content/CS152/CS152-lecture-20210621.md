---
title: "CS152-lecture-20210621"
# date: 2021-06-21T11:07:05-07:00
draft: false
bookToc: true
tags: ["hashing", "sponge construction", "reductions"]
---

## Cryptographic hash functions

Hashing takes a very large domain and maps it to a smaller codomain.
For this to scale nicely,

- the hash function must be fast
- the outputs must be fairly random in distribution

A hash function can be defined as {{<k>}} H : \{0,1\}^* \to \{0,1\}^b {{</k>}}, where {{<k>}} b {{</k>}} is the output block length.

{{< hint info >}}
Recall: a set raised to an asterisk means the set of all strings made from that set.
{{< /hint >}}

We insist a cryptographic hash function has these properties:

- **Preimage resistance**: it should be non invertible. If we know an output, we shouldn't be able to find an input that produces that output.
Given {{<k>}} y {{</k>}}, it is hard to find any {{<k>}} x {{</k>}} such
such that {{<k>}} H(x) = y {{</k>}}.
This should just be really hard to find (near impossible).
Note that since we have an infinite input, there definitely are multiple values that map to the same output value, but it should be near impossible to find them.
- **Second preimage resistance**: if we have another value, it should be hard to find another value that map to the same output.
Given {{<k>}} x_1 {{</k>}}, it is hard to find {{<k>}} x_2 \neq x_1 {{</k>}} such 
that {{<k>}} H(x_1) = H(x_2) {{</k>}}
- **Collision resistance**: it is hard to find **any** pair that cause the same output.
It is  hard to find {{<k>}} x_2 \neq x_1 {{</k>}} such
that {{<k>}} H(x_1) = H(x_2) {{</k>}}
- A nice to have property: behave like a public random function (random oracle).

Since hash algorithms aren't invertible, they are useful for
- fingerprints
- message authenticity, if two the sender's and receiver's copy of the message being sent both hash to the same value, it can be considered authentic
- some file systems use hash functions to determine if a file has been changed

### Some history for hash functions

MD5
- Invented in the 90s
- block size of {{<k>}} b = 128 {{</k>}} bits
- MD5 is now broken, as pairs of input which produce the same output can be found rather easily now.

SHA-1
- Invented in the 90s
- block size of {{<k>}} b = 160 {{</k>}} bits
- broken, deprecated

SHA-2
- a fixed version of SHA-1, considered secure
- also called SHA-256, SHA-512
- block size can be {{<k>}} b = 256 {{</k>}} or {{<k>}} 512 {{</k>}}, respectively.

SHA-3
- competition to find next best hash algorithm
- Same style contest as AES, winner was Keccak
- block size {{<k>}} b = 256 {{</k>}} or {{<k>}} 512 {{</k>}}
- considered secure

### Telephone coin flipping

Imagine a friend flipping a coin on the other end of a phone call.
Your friend may think that you may cheat and have no way of knowing.
In fact, you could cheat and your friend wouldn't know.

How could they use a hash function to ensure no one is lying?

- let person {{<k>}} A {{</k>}} pick a random {{<k>}} x {{</k>}}, 128 bits
- {{<k>}} A {{</k>}} tells {{<k>}} B {{</k>}} the hash value {{<k>}} H(x) {{</k>}} 
- {{<k>}} B {{</k>}} chooses 0 or 1, and tells {{<k>}} A {{</k>}}
- {{<k>}} A {{</k>}} tells {{<k>}} B {{</k>}} the value {{<k>}} x {{</k>}}
- consider the last bit of {{<k>}} x {{</k>}} to be the answer to the coin flip, {{<k>}} 0 {{</k>}} = heads and {{<k>}} 1  {{</k>}} = tails.
- {{<k>}} B {{</k>}} wins if last bit of {{<k>}} x {{</k>}} matches guess

By sending {{<k>}} H(x) {{</k>}}, {{<k>}} A {{</k>}} is commiting to the input {{<k>}} x {{</k>}}.
It is hard for {{<k>}} B {{</k>}} to find the input {{<k>}} x {{</k>}} when they only know the hash value {{<k>}} H(x) {{</k>}},
so it is hard for {{<k>}} A {{</k>}} or {{<k>}} B {{</k>}} to cheat.

## Hash constructions

Recall the signature of a cryptographic hash function,

{{<k display>}} H : \{0,1\}^* \to \{0,1\}^b {{</k>}}

where {{<k>}} b {{</k>}} is the block size.

### Merkle–Damgård construction

{{< hint info >}}
[*Merkle–Damgård wikipedia*](https://en.wikipedia.org/wiki/Merkle–Damgård_construction) 
{{< /hint >}}

Since this function doesn't take any key, you just feed it data and it returns the result of the hash function.

There are some initial constants that are concatenated with a chunk of the input, and that is fed into a compression function.

![image_2021-06-21-12-32-45](/notes/image_2021-06-21-12-32-45.png)

The output of this function is fed into the next compression function,

![image_2021-06-21-12-34-06](/notes/image_2021-06-21-12-34-06.png)

The last block must be padded to a multiple of the block size, in this case {{<k>}} b = 512 {{</k>}}.
We will use our `10*` padding algorithm for the last block - 64 bits.
The length is concatenated onto the end

![image_2021-06-21-12-36-33](/notes/image_2021-06-21-12-36-33.png)

The output of the last call to the compression function {{<k>}} f {{</k>}} is the hash output, and we can think of {{<k>}} f {{</k>}} as a public random function.

So, {{<k>}} f {{</k>}} should be

- collision resistant
- preimage/second preimage resistant

Compression functions like {{<k>}} f {{</k>}} usually have

- lots of rounds
- mostly bitwise operation

{{< hint info >}}
Read more about the compression function in SHA-1 [here](https://en.wikipedia.org/wiki/SHA-1).
{{< /hint >}}

MD5, SHA-1,2 use this same basic construction.
Since these hash functions are considered broken.

Also, since the output is the last call to the hash function, it opens up the function to length extension attacks.

### Keccak (SHA-3) – sponge construction

{{< hint info >}}
Read about the SHA-3 algorithm [here](https://en.wikipedia.org/wiki/SHA-3).
{{< /hint >}}

The padding at the last block of the data is a `10*1` algorithm.
The data must be a multiple of {{<k>}} r {{</k>}} bits.

![image_2021-06-21-12-56-27](/notes/image_2021-06-21-12-56-27.png)

How it works:

- starts by creating a block of {{<k>}} r {{</k>}} bits and {{<k>}} c {{</k>}} bits, where
{{<k>}} r {{</k>}} is rate and {{<k>}} c {{</k>}} is capacity, initialized to 0
![image_2021-06-21-12-57-33](/notes/image_2021-06-21-12-57-33.png)
- split data into multiples of {{<k>}} r {{</k>}}
- take the first block and xor it with current block
![image_2021-06-21-12-58-18](/notes/image_2021-06-21-12-58-18.png)
This leaves the low {{<k>}} c {{</k>}} bits alone.
- the output of this goes through a permutation function {{<k>}} P {{</k>}}
![image_2021-06-21-12-58-57](/notes/image_2021-06-21-12-58-57.png)
{{<k>}} P {{</k>}} should be thought of as a public random permutation.
- the output of this is xor'd with the next {{<k>}} r {{</k>}} bits of the data, processed through {{<k>}} P {{</k>}}, and this continues
![image_2021-06-21-13-00-13](/notes/image_2021-06-21-13-00-13.png)
This continues until the data is exhausted.
- The output is fed through {{<k>}} P {{</k>}} one last time, and the result is the high {{<k>}} r {{</k>}} bits of output.

Since the side of {{<k>}} c {{</k>}} bits is never touched, nobody has access to manipulating those {{<k>}} c {{</k>}} bits.
The only way to manipulate the function is to change the bits in {{<k>}} r {{</k>}}, which is changing the data.

If our permutation doesn't output {{<k>}} r {{</k>}} bits, it can be called a couple of times until we have {{<k>}} r {{</k>}} bits to return.

- The hash function's speed is effected by how big {{<k>}} r {{</k>}} is (which is why we define {{<k>}} r {{</k>}} as rate).
Each invocation of {{<k>}} P {{</k>}} consumes {{<k>}} r  {{</k>}} bits.
- {{<k>}} c {{</k>}} sets the security.
An attack will try to cause the {{<k>}} c {{</k>}} bits of the block to collide.
A secure hash function must have a large enough {{<k>}} c {{</k>}} side to make collisions very unlikely.
To be secure nowadays, {{<k>}} c \geq 256 {{</k>}}.

Consider the last block,

- students often copy the entire data into a buffer that can hold the original data plus the buffered data
    - this is extremely inefficient, imagine a buffer that is 10GB.
- we'll never have to copy the first multiple of {{<k>}} r {{</k>}} bit
    - instead we'll xor the first {{<k>}} r {{</k>}} bits
    into our chaining value, until we get to the last block
    - for the tail, we'll use a temporary buffer that is allocated on the stack,
    and copy the bytes from data then do the `10*1` padding.
    - that way we don't have to make a copy of the entire data

Later on, when we do a `perm384` sponge construction, we will use values of
- {{<k>}} r = 128 {{</k>}} bits
- {{<k>}} c = 256 {{</k>}} bits

If we want our output to be {{<k>}} b = 256 {{</k>}} bits, we will have to call our permutation at the end twice (since our
`perm384` outputs 16 bytes at a time).

## Sponge construction example

Let {{<k>}} P : \{0,1\}^6 \to \{0,1\}^6 {{</k>}} and our permutation function as
{{<k>}} P(x) = \text{ROTL(x,1)} {{</k>}}

Lets define our rate and capacity as {{<k>}} r = c = 3 {{</k>}}.

Recall that the sponge starts as all 0s, split into {{<k>}} r {{</k>}} and {{<k>}} c {{</k>}} bits:

![image_2021-06-21-13-18-26](/notes/image_2021-06-21-13-18-26.png)

If we want to hash the value

```
110011 0101
```

we'll need to pad the last block using our `10*1` algorithm,

```
110011 010111
```

{{< hint info >}}
Note: we always have to add the two 1s when we pad, the 0s are the only thing that is optional.
{{< /hint >}}

Next we xor the first 3 bits of our data and xor it with our sponge,

![image_2021-06-21-13-21-02](/notes/image_2021-06-21-13-21-02.png)

Then we send the sponge through the permutation,

![image_2021-06-21-13-21-19](/notes/image_2021-06-21-13-21-19.png)

This continues onto the next block of {{<k>}} r {{</k>}} bits:

![image_2021-06-21-13-22-24](/notes/image_2021-06-21-13-22-24.png)
![image_2021-06-21-13-22-57](/notes/image_2021-06-21-13-22-57.png)
![image_2021-06-21-13-23-29](/notes/image_2021-06-21-13-23-29.png)

This is the end of the absorbing phase.
Next is the squeezing phase, this takes the first {{<k>}} r {{</k>}} bits and outputs it as the hash output.
So if we want our output to be 6 bits, we'll outout as many of the {{<k>}} r {{</k>}} bits as necessary.
So we can output the first {{<k>}} 3 {{</k>}} bits, and then send it through the permutation again.

![image_2021-06-21-13-26-28](/notes/image_2021-06-21-13-26-28.png)

{{< hint info >}}
Note: This isn't the best example because the output of `ROTL(111111)` is indistinguishable from the input.
{{< /hint >}}

## Reductions

If you can write an algorithm that uses a subroutine, like

```
ASolver(a_inst):
    b_inst = convert_a_to_b(a_inst)
    b_sol = BSolver(b_inst)
    a_sol = convert_b_to_a(b_sol)
    return a_sol
```

This means that `ASolver` reduces to `BSolver`, which means that

{{<k display>}}
\begin{aligned}
    \exists \text{BSolver} \to \exists \text{ASolver}
\end{aligned}
{{</k>}}

"If `BSolver` exists, then `ASolver` exists"

These implications do not make any assumptions about the true/false of their existence, it just merely shows an assertion should it exist.

It sometimes happens that we are interested in the constrapositive:

{{<k display>}}
\begin{aligned}
    \neg \exists \text{ASolver} \to \neg \exists \text{BSolver}
\end{aligned}
{{</k>}}

"If `ASolver` does not exist, then `BSolver` does not exist"

### `FindMedian` reduction example

```
// arr could be in any order
FindMedian(arr):
    arr' = Sort(arr)
    ans = arr'[len(arr') / 2] // assumes odd length arr
```

{{<k display>}}
\begin{aligned}
    \exists \text{Sort} &\to \exists \text{FindMedian} \\
    \neg \exists \text{FindMedian} &\to \neg \exists \text{Sort}
\end{aligned}
{{</k>}}

This shows that `FindMedian` reduces to `Sort`.

{{< hint info >}}
Note: If we say `FindMedian` doesn't exist, it means that it doesn't exist for **all** input.
{{< /hint >}}

### Reductions in cryptography

Reductions are important in cryptography because
- one of the ways that cryptographic protocols are proven secure is using these reductions

Since we could use a specific block cipher in an encryption mode, the security of the mode reduces to the security of the block cipher.

### `CollisionFinder` reduction example

```
// returns x1 != x2 such that H(x1) = H(x2)
CollisionFinder(H):
    x1 = random string
    x2 = 2ndPreimageFinder(H, x1)
    return (x1, x2)
```

{{<k display>}}
\begin{aligned}
    \exists \text{2ndPreimageFinder} &\to \exists \text{CollisionFinder} \\
    \neg \exists \text{CollisionFinder} &\to \neg \exists \text{2ndPreimageFinder}
\end{aligned}
{{</k>}}

The contrapositive is a more interesting statement: "Collision resistance implies 2nd preimage resistance."

There should be an infinite number of {{<k>}} x_1 {{</k>}}
that map to {{<k>}} x_2 {{</k>}}, if the permutation is truly random.
But the hash function may not be a perfect permutation, so you could argue to wrap the 2 lines of code in `CollisionFinder` in a loop.
This would change the string and attempt to find new matches.

### `PreimageFinder` example

```
CollisionFinder(H):
    do:
        x1 = random string
        y = H(x1)
        x2 = PreimageFinder(H, y) // H(x) = y
    while x2 = NULL or x1 = x2
    return (x1, x2)
```

{{<k display>}}
\begin{aligned}
    \exists \text{PreimageFinder} &\to \exists \text{CollisionFinder} \\
    \neg \exists \text{CollisionFinder} &\to \neg \exists \text{PreimageFinder}
\end{aligned}
{{</k>}}

"Collision resistance implies preimage resistance."

So this displays that the ultimate goal should be collision resistance, because it will also cover preimage/2nd preimage resistance.

## Universal hashing

Compared to cryptographic hashes, which are over engineered for security, universal hashes are a lot more efficient.
We don't always need collision resistance and random looking output.
What we sometimes need is a bound on the probability that the adversary could find a pair of inputs that produce the same output.

We can optimize universal hash functions to meet the goals needed. 

Its good to note that universal hashes are
- fast, and
- provable to have a small probability to find collisions

This last point when compared to cryptographic hash functions: cryptographic hash functions are overengineered and have so many heuristics that its harder to prove the probability of collisions.

Universal hash functions do not have as much guarantee of the crypgoraphic hash functions, but if all we need is the probabilistic property, then they are the way to go.

### Abstract epsilon universal hash example

Let {{<k>}} H {{</k>}} be a collection of hash functions {{<k>}} A \to B {{</k>}}, (from some domain {{<k>}} A {{</k>}} to a codomain {{<k>}} B {{</k>}}).
{{<k>}} H {{</k>}} is {{<k>}} \epsilon {{</k>}}-almost-universal if the probability of winning the following game is {{<k>}} \leq \epsilon {{</k>}}.

The game is described as

1. Adversary chooses {{<k>}} a {{</k>}} and {{<k>}} b {{</k>}}, where {{<k>}} a \neq b \in A{{</k>}}.
2. {{<k>}} h \in H {{</k>}} is chosen randomly. (Note, the adversary knows the functions in {{<k>}} H {{</k>}} but doesn't know which one has been picked.)
- Adversary wins if
    - {{<k>}} h(a) = h(b) {{</k>}}

### Concrete universal hash example

Consider this mapping {{<k>}} Z_6 \to Z_4 {{</k>}}:

![image_2021-06-21-14-48-20](/notes/image_2021-06-21-14-48-20.png)

This is a small enough sample of objects that we can calculate the probability of winning the game:
- {{<k>}} (0,1) = \frac{1}{4} {{</k>}}
- {{<k>}} (0,2) = 0 {{</k>}}
- {{<k>}} (0,3) = 0 {{</k>}}
- {{<k>}} (0,4) = 0 {{</k>}}
- {{<k>}} (0,5) = \frac{1}{4} {{</k>}}

We can continue to go through these pairs until we find a probability that is better than {{<k>}} \frac{1}{4} {{</k>}}, for example

- {{<k>}} (1,3) = \frac{1}{2} {{</k>}}
- {{<k>}} (2,4) = \frac{1}{2} {{</k>}}

Since we can win this game, no more than {{<k>}} \frac{1}{2}  {{</k>}} the time, we consider it to be {{<k>}} \epsilon {{</k>}}-almost-universal for {{<k>}} \epsilon = \frac{1}{2} {{</k>}}.

So the smaller the epsilon {{<k>}} \epsilon {{</k>}}, the less probability of a collision.

