<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CS177 on Notes</title>
    <link>http://zedchance.github.io/notes/CS177/</link>
    <description>Recent content in CS177 on Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="http://zedchance.github.io/notes/CS177/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CS177-lecture-20220125</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220125/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220125/</guid>
      <description>Data Mining #  Syllabus #  File: 177-syllabus.pdf  Notes during orientation #   Programming assignments will be in Python Most of the programming assignments are from past TAs Fundamental machine learning Slides are in files on canvas Book resources Lab attendance is required, .ipynb files on canvas  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220127</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220127/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220127/</guid>
      <description>Exploring data #  Ultimately, any machine learning is just optimizing error in a graph, then deriving a function that fits to the data.
Read more about John Tukey
Iris sample data set #   http://www.ics.uci.edu/~mlearn/MLRepository.html  Statistics definitions #   ordinal means that the data can be ordered continuous means that the values are floating point      \( \text{AAD} (x) \)  is the absolute distance  \( \text{MAD} (x) \)  is the median dependent value  Visualization #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220202</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220202/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220202/</guid>
      <description>Exploratory analysis cont. #  Representing data cont. #  Analyzing data #  Mining introduction #  Tasks #   anomaly detection is relatively new  Classification #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220203</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220203/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220203/</guid>
      <description>Data exploration cont. #  Classification cont. #  Regression #  Clustering #  Association #  Deviation / anomaly / change detection #  Challenges #   the concept of neural networks has been around for a long time, however only because of recent computational power has it become widely used  What is data? #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220210</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220210/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220210/</guid>
      <description>NumPy #  We can use NumPy with
import numpy as np We can create new arrays with
a = np.array([1, 5, 7]) We can specify number of dimensions or data types:
b = np.array([1, 2, 3], ndim=2) c = np.array([1, 2, 3, 4], dtype=complex) We can see the shape of an array via
print(a) print(a.shape) You can set the shape of an array
d = np.array([1, 2, 3], [4, 5, 6]) d.</description>
    </item>
    
    <item>
      <title>CS177-lecture-20220215</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220215/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220215/</guid>
      <description>What is data? #  Attributes #  Difference between ratio and interval #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220217</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220217/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220217/</guid>
      <description>Data preprocessing cont. #  Attributes cont. #  Types of data sets #  Data quality #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220222</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220222/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220222/</guid>
      <description>Data quality cont. #  Distance #  Similarity #  Correlation #  Measures #  Density #  Preprocessing #  Sampling #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220224</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220224/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220224/</guid>
      <description>Common procedures for preprocessing #   Aggregation Dimension Reduction Standardization Missing values Bad data (noise reduction) Translation (log) Feature extraction Removing outliers  Classification #  Hunt&amp;rsquo;s algorithm #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220301</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220301/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220301/</guid>
      <description>To reduce dimensions #  Use PCA and SVD.
Classification cont. #  Methods of splitting #  How to determine the best split #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220303</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220303/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220303/</guid>
      <description>Classification cont. #  Splitting cont. #  Decision tree based classification #  </description>
    </item>
    
    <item>
      <title>CS177-lecture-20220308</title>
      <link>http://zedchance.github.io/notes/CS177/CS177-lecture-20220308/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://zedchance.github.io/notes/CS177/CS177-lecture-20220308/</guid>
      <description>Linear regression #  Best practices #  </description>
    </item>
    
  </channel>
</rss>
